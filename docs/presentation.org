#+title: Empirical Finance - Research Proposal
#+SUBTITLE: Finding Scaling Laws in Financial data
#+Bibliography: local-bib.bib
#+BEAMER_FRAME_LEVEL: 2
#+OPTIONS: H:4 toc:2 num:nil
#+EMAIL: jiyan.schneider@keio.jp
#+AUTHOR: Jiyan Schneider
#+OPTIONS: reveal_history:t reveal_fragmentinurl:t
#+OPTIONS: reveal_hash:nil

:REVEAL_PROPERTIES:
#+REVEAL_TITLE_SLIDE: <h1>%t</h1><br><h2>%s</h2><br><h4>%a</h4><br>Keio University, Graduate school of Economics
# #+REVEAL_ROOT: ./reveal.js/
#+REVEAL_ROOT: https://cdn.jsdelivr.net/npm/reveal.js
#+REVEAL_EXTRA_CSS: custom.css
#+REVEAL_THEME: serif
#+REVEAL_TRANS: linear
#+REVEAL_HLEVEL: 1
:END:

* Introduction
:PROPERTIES:
:CUSTOM_ID: introduction
:ID:       aef04778-f1c7-4525-93e7-623cb2b98d56
:END:
#+BEGIN_NOTES
- Introduction
  – “AI progress became predictable once we knew the scaling law.”
  – Ask yourself: could finance become predictable the same way?
#+END_NOTES
  - Recent breakthroughs in AI, particularly Large Language Models (LLMs), have been driven by the "scaling hypothesis."
  - **Scaling Hypothesis:** Performance of a model predictably improves as we increase resources (model size, data, compute).
  - This relationship often follows a smooth power law, making progress measurable and forecastable.
  - **Research Question:** Can similar scaling laws be identified in the domain of quantitative finance?

#+ATTR_HTML: :height 200px
[[file:assets/scaling_laws_improvement.jpg]]

Taken from [cite/text:@kaplanScalingLawsNeural2020]

* Proposal of idea
:PROPERTIES:
:CUSTOM_ID: Proposal of idea
:END:
#+BEGIN_NOTES
  – We systematically hunt for a power-law L(N,D) in the 10-second mid-price prediction task.
  – If it exists, it answers the ROI question forever.

#+END_NOTES
  - This research proposes to systematically investigate the existence of scaling laws in financial forecasting.
  - We test the hypothesis that a financial model's performance (e.g., predictive accuracy) scales as a power-law with:
    1. **Model Size (N):** The number of parameters in the model.
    2. **Data Size (D):** The volume of historical data used for training.
  - The goal is to fit an empirical formula, similar to those found in LLMs:
    \[ L(N, D) \approx E_{\text{irred}} + \frac{A}{N^\alpha} + \frac{B}{D^\beta} \]
    where L is the prediction loss.
**** Results
:PROPERTIES:
:CUSTOM_ID: results_expected
:ID:       8aec029f-348d-45b8-8b5f-3466658ea20a
:END:
#+BEGIN_NOTES
 - This slide shows what the output of our resarch would be
 - This is from Hoffman paper for scaling laws of LLMs. Since this is just a proposal, I will put it here, explain it
 - so that you could understand what we would get.
#+END_NOTES

[[file:assets/Learning predictions.jpg]]
Figure taken from [cite/text:@hoffmannTrainingComputeOptimalLarge2022]
* Motivation
:PROPERTIES:
:CUSTOM_ID: motivation
:ID:       2bc86fef-6214-4ecd-8a9f-99b4c5756e2c
:END:
#+BEGIN_NOTES
 – This lens already guided $100 M decisions at OpenAI, Google, DeepSeek.
 – The same tool can tell us when to buy Compute vs. do data acquisition
 - We could answer questions like, How much data do we need to achieve our goals?
 - Do we need more compute given our data?
 - Is what we want to do even realistically achievable with our data / all available data on the planet
 - Lastly, its also feasible
#+END_NOTES
  - **Practical:** If scaling laws hold, we can optimize resource allocation.
    - Avoids costly trial-and-error in model development.
    - Allows us to forecast the "return on investment" for acquiring more data or training larger models.
    - Informs optimal model design, analogous to how Hoffmann et al. (2022) found smaller models trained on more data were optimal.
  - **Theoretical:**
    - Provides a new framework for understanding the limits of predictability in financial markets.
    - The "irreducible error" term ($E_{irred}$) in the scaling law could be interpreted as a measure of fundamental market efficiency or noise.
  - **Feasible:**
    - Scaling laws seem to hold even on quite small scales so you don't need a 1024 H100 machines.
* Background
:PROPERTIES:
:CUSTOM_ID: background
:END:

** Initial Scaling law research
:PROPERTIES:
:CUSTOM_ID: initial_scaling_law_research
:END:
#+BEGIN_NOTES
- Background (Kaplan, left)
  – First empirical evidence that language performance scales smoothly with model / data / compute.
  – Implied: “train the largest model you can afford.”

- Background (Hoffmann, right) – IsoFLOP revelation
  – Their famous figure: U-shaped iso-Flops.
  – Minimum was 4× smaller model + 4× more data → the cheaper, stronger “Chinchilla.”
  – Our task: find the financial “Chinchilla.”
#+END_NOTES
  - **Kaplan et al. (2020), "Scaling Laws for Neural Language Models"** [cite:@kaplanScalingLawsNeural2020]
    - First to comprehensively demonstrate that LLM loss scales smoothly as a power-law with model size, dataset size, and training compute.
    - Their findings suggested that for best performance, model size should be the primary focus of scaling efforts.
  - **Hoffmann et al. (2022), "Training Compute-Optimal Large Language Models"** [cite:@hoffmannTrainingComputeOptimalLarge2022]
    - Refined Kaplan's work. By their analysis, for optimal performance under a fixed compute budget, model size and dataset size should be scaled in roughly equal proportion.
    - Their model, "Chinchilla," though much smaller than competitors like Gopher (or GPT-3), outperformed them by being trained on significantly more data.
  - We want to apply the methodologies of these papers to a financial context
** Current research
:PROPERTIES:
:CUSTOM_ID: current-research
:END:
#+BEGIN_NOTES
 - Currently scaling law research is done for new architectures / types of data
This isn't just a theoretical exercise.
 - Guide massive investments in Large Language Models -> The Kaplan 2020 paper is said to be what inspired OpenAI to try and go for ChatGPT
 - Hoffman was at google (gemini)
 - The last author of the Kaplan paper was Amodei (Now at anthropic)
#+END_NOTES
   - The "scaling" approach is a proven framework for making progress in complex domains by making it predictable and measurable.
   - It has been used to:
     - Guide massive investments in Large Language Models. [cite:@kaplanScalingLawsNeural2020]
     - Discover optimal model architectures and data-to-model size ratios [cite:@hoffmannTrainingComputeOptimalLarge2022;@deepseek-aiDeepSeekLLMScaling2024]
     - Achieve breakthroughs in computer vision. [cite:@zhaiScalingVisionTransformers2022]
     - The existence of bad scaling laws with audio cause pessism and leads people to pivot into new research directions [cite:@cuervoScalingPropertiesSpeech2024;@maimonScalingAnalysisInterleaved2025]

   - **Our goal is to bring these same benefits to quantitative finance:**
     - **Practical:** Forecast the ROI of compute/data, optimize resource allocation, and avoid costly trial-and-error.
     - **Theoretical:** Create a new framework for measuring market predictability and estimating the "irreducible" market noise ($E_{\text{irred}}$).

* Methodology
:PROPERTIES:
:CUSTOM_ID: methodology
:END:

** Data
:PROPERTIES:
:CUSTOM_ID: data
:END:
#+BEGIN_NOTES
  - Considered multiple time horizons, however due to the need for Big data, short time horizons seem better
  - If we find something on long time horizons, we won't have enough data to scale it infinitely anyways.
  – S&P 500 TAQ 2010–2023.
  – Task: predict sign and magnitude of ∆ mid-Price over next 10 s from previous 60 s.
  – 7 TB compressed = enough samples to actually scale.
#+END_NOTES
  - We use high-frequency trade and quote (TAQ) data for a broad set of US equities (e.g., S&P 500 components).
  - **Period:** 2010-2023, providing a large dataset to sample from.
  - **Prediction Task:** Predict the sign and magnitude of the mid-price change over the next 10 seconds based on the previous 60 seconds of order book and trade data. This is a simple, well-defined, and computationally tractable task.
  - Only train for a single Epoch at most
  - Extensively care that we do not use future data to evaluate our models
** Methods
:PROPERTIES:
:CUSTOM_ID: methods
:END:
#+BEGIN_NOTES
- IsoFLOP design – one curve per slide
  1. Pick total FLOP budget   → horizontal dashed line in slide.
  2. Train 20 models at the same FLOP but different (N,D) splits.
  3. Plot Loss vs. N to get the U-shape.
  4. Repeat for three budgets = three U-curves.
  5. The bottom points trace N_opt(D_opt) → ratio of α/β.
#+END_NOTES

#+begin_leftcol
Use the ISOFlop approach (Approach 2 from [cite/text:@hoffmannTrainingComputeOptimalLarge2022])
   - **Define Model Architecture:**
     - A simple, scalable state of the art neural network. (Transformer-based, decoder only, unidirectional)
   - **Define Compute Budgets:**
   - **Pick compute budget:** number of FLOPs  E.g., \( 10^{18}, 10^{19}, 10^{20 } \) FLOPs
   - **Model Training & Evaluation:** Train slighly different models on Slightly different amounts of data, record final loss.
   - **Analysis:** Plot the final loss for each model against its size and estimate the log-log relationship
   - **If possible:** Train a final big model and log-log relationship prediction to actual performance
#+end_leftcol

*** Transformer mdoel
#+BEGIN_NOTES
- Studied a lot
- In finance as well
- Felt low risk
- Easily can adapt width and height parameters
#+END_NOTES

#+ATTR_HTML: :center :height 460px
[[file:assets/transformer_arch.jpg]]

Figure taken from [cite/text:@vaswani17_atten_is_all_you_need]

*** Isoflop Curve
#+BEGIN_NOTES
 - After the experiment is done I want to plot the isocurves like this.
#+END_NOTES

:PROPERTIES:
:ID:       43c3d472-8232-4a30-b041-eb5ec8d62344
:END:
[[file:assets/isoflop.jpg]]
Figure taken from [cite/text:@deepseek-aiDeepSeekLLMScaling2024]

* Method of evaluation
:PROPERTIES:
:CUSTOM_ID: methods-of-evaluatoin
:END:
  - /Forecast error/: final cross-entropy vs baseline logit model (y-axis log-transformed).
  - /Economic value/: out-of-sample Sharpe ratio of a pure signal-driven strategy.
  - Increasing \( R^{2} \)s in the log-log relationship would validate the scaling hypothesis.
  - However considering other
  - We analyze the estimated exponents \( \alpha \) and \( \beta \). These determine the relative importance of model size vs. data size for financial prediction.
  - As a secondary, economic evaluation, we can construct a simple trading strategy based on the predictions of the best models to gauge their potential profitability (e.g., Sharpe ratio).
* Limitations and plans
:PROPERTIES:
:CUSTOM_ID: limitations
:END:
#+BEGIN_NOTES
#+END_NOTES
  - **Computational Cost:** Training a large grid of models is resource-intensive.
    - **Plan:** Start with a smaller-scale pilot study. Utilize university high-performance computing (HPC) resources or cloud credits.
  - **Evalution Criteria:** Is our current criteria \( R^{2} \) actually amenable for these scaling laws?
  - **Data Non-stationarity:** Financial markets evolve, which may complicate the scaling relationship.
  - **Generalizability:** Results might be specific to our chosen task, data, or model architecture.
    - **Plan:** If time permits, test a secondary task (e.g., volatility prediction) or a different model family (e.g., an LSTM).

* Bibliography
:PROPERTIES:
:CUSTOM_ID: bibliography
:END:
#+CITE_EXPORT: csl chicago-author-date-without-url.csl
#+print_bibliography:

