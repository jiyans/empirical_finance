@misc{deepseek-aiDeepSeekLLMScaling2024,
  title = {{{DeepSeek LLM}}: {{Scaling Open-Source Language Models}} with {{Longtermism}}},
  shorttitle = {{{DeepSeek LLM}}},
  author = {{DeepSeek-AI} and Bi, Xiao and Chen, Deli and Chen, Guanting and Chen, Shanhuang and Dai, Damai and Deng, Chengqi and Ding, Honghui and Dong, Kai and Du, Qiushi and Fu, Zhe and Gao, Huazuo and Gao, Kaige and Gao, Wenjun and Ge, Ruiqi and Guan, Kang and Guo, Daya and Guo, Jianzhong and Hao, Guangbo and Hao, Zhewen and He, Ying and Hu, Wenjie and Huang, Panpan and Li, Erhang and Li, Guowei and Li, Jiashi and Li, Yao and Li, Y. K. and Liang, Wenfeng and Lin, Fangyun and Liu, A. X. and Liu, Bo and Liu, Wen and Liu, Xiaodong and Liu, Xin and Liu, Yiyuan and Lu, Haoyu and Lu, Shanghao and Luo, Fuli and Ma, Shirong and Nie, Xiaotao and Pei, Tian and Piao, Yishi and Qiu, Junjie and Qu, Hui and Ren, Tongzheng and Ren, Zehui and Ruan, Chong and Sha, Zhangli and Shao, Zhihong and Song, Junxiao and Su, Xuecheng and Sun, Jingxiang and Sun, Yaofeng and Tang, Minghui and Wang, Bingxuan and Wang, Peiyi and Wang, Shiyu and Wang, Yaohui and Wang, Yongji and Wu, Tong and Wu, Y. and Xie, Xin and Xie, Zhenda and Xie, Ziwei and Xiong, Yiliang and Xu, Hanwei and Xu, R. X. and Xu, Yanhong and Yang, Dejian and You, Yuxiang and Yu, Shuiping and Yu, Xingkai and Zhang, B. and Zhang, Haowei and Zhang, Lecong and Zhang, Liyue and Zhang, Mingchuan and Zhang, Minghua and Zhang, Wentao and Zhang, Yichao and Zhao, Chenggang and Zhao, Yao and Zhou, Shangyan and Zhou, Shunfeng and Zhu, Qihao and Zou, Yuheng},
  year = {2024},
  month = jan,
  number = {arXiv:2401.02954},
  eprint = {2401.02954},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.02954},
  urldate = {2025-03-14},
  abstract = {The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling laws described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate the scaling of large scale models in two prevalent used opensource configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and direct preference optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B across a range of benchmarks, especially in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that our DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/jjs/Zotero/storage/BA2NV7YY/DeepSeek-AI et al. - 2024 - DeepSeek LLM Scaling Open-Source Language Models with Longtermism.pdf;/Users/jjs/Zotero/storage/KYG848F2/DeepSeek-AI et al. - 2024 - DeepSeek LLM Scaling Open-Source Language Models with Longtermism.pdf;/Users/jjs/Zotero/storage/MYQXIMX5/DeepSeek-AI et al. - 2024 - DeepSeek LLM Scaling Open-Source Language Models .pdf;/Users/jjs/Zotero/storage/6T2X9LVP/2401.html;/Users/jjs/Zotero/storage/W53ANMMB/2401.html}
}

@misc{hoffmannTrainingComputeOptimalLarge2022,
  title = {Training {{Compute-Optimal Large Language Models}}},
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
  year = {2022},
  month = mar,
  number = {arXiv:2203.15556},
  eprint = {2203.15556},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.15556},
  urldate = {2025-03-21},
  abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/jjs/Zotero/storage/LMLYPDP6/Hoffmann et al. - 2022 - Training Compute-Optimal Large Language Models.pdf}
}
@misc{kaplanScalingLawsNeural2020,
  title = {Scaling {{Laws}} for {{Neural Language Models}}},
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  year = {2020},
  month = jan,
  number = {arXiv:2001.08361},
  eprint = {2001.08361},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2001.08361},
  urldate = {2025-06-04},
  abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/jjs/Zotero/storage/LQERJ8NU/Kaplan et al. - 2020 - Scaling Laws for Neural Language Models.pdf;/Users/jjs/Zotero/storage/3D3LFZA5/2001.html}
}

@inproceedings{zhaiScalingVisionTransformers2022,
  title = {Scaling {{Vision Transformers}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhai, Xiaohua and Kolesnikov, Alexander and Houlsby, Neil and Beyer, Lucas},
  date = {2022-06},
  pages = {1204--1213},
  publisher = {IEEE},
  location = {New Orleans, LA, USA},
  doi = {10.1109/cvpr52688.2022.01179},
  url = {https://ieeexplore.ieee.org/document/9880094/},
  urldate = {2025-07-16},
  abstract = {Attention-based neural networks such as the Vision Transformer (ViT) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, understanding a modelâ€™s scaling properties is a key to designing future generations effectively. While the laws for scaling Transformer language models have been studied, it is unknown how Vision Transformers scale. To address this, we scale ViT models and data, both up and down, and characterize the relationships between error rate, data, and compute. Along the way, we refine the architecture and training of ViT, reducing memory consumption and increasing accuracy of the resulting models. As a result, we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45\% top-1 accuracy. The model also performs well for few-shot transfer, for example, reaching 84.86\% top-1 accuracy on ImageNet with only 10 examples per class.},
  eventtitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  langid = {english},
  file = {/Users/jjs/Zotero/storage/F8UV8R7B/Zhai et al. - 2022 - Scaling Vision Transformers.pdf}
}
@article{guEmpiricalAssetPricing2020,
  title = {Empirical {{Asset Pricing}} via {{Machine Learning}}},
  author = {Gu, Shihao and Kelly, Bryan and Xiu, Dacheng},
  editor = {Karolyi, Andrew},
  date = {2020-05-01},
  journaltitle = {The Review of Financial Studies},
  volume = {33},
  number = {5},
  pages = {2223--2273},
  publisher = {Oxford University Press (OUP)},
  issn = {0893-9454, 1465-7368},
  doi = {10.1093/rfs/hhaa009},
  url = {https://academic.oup.com/rfs/article/33/5/2223/5758276},
  urldate = {2025-07-20},
  abstract = {Abstract               We perform a comparative analysis of machine learning methods for the canonical problem of empirical asset pricing: measuring asset risk premiums. We demonstrate large economic gains to investors using machine learning forecasts, in some cases doubling the performance of leading regression-based strategies from the literature. We identify the best-performing methods (trees and neural networks) and trace their predictive gains to allowing nonlinear predictor interactions missed by other methods. All methods agree on the same set of dominant predictive signals, a set that includes variations on momentum, liquidity, and volatility.               Authors have furnished an Internet Appendix, which is available on the Oxford University Press Web site next to the link to the final published paper online.},
  langid = {english},
  file = {/home/jjs/Zotero/storage/5W8K7BUH/Gu et al. - 2020 - Empirical Asset Pricing via Machine Learning.pdf}
}
